---
layout: page
title: Physicist & Data Scientist | Expert on Machine-Learning 
---
![screenshot](images/artificial_intelligence.png)

## My Personal Background
I am a former researcher and now passionate data scientist with more than 10 years of experience in academic research applying profound knowledge in medical physics, maths and statistics to solve many real-world challenges in applied medical imaging and signal processing. After academia, I gained a wide range of experience in the renewable energy industry in the role of a data scientist consultant and machine learning (ml) developer. I consulted various departments on developing data science solution to solve real-world business problems. I also implemented numerous ml and deep-learning (dl) models in proof-of-concept (PoC) studies.

My favourite quote:
*“It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.” ― Richard P. Feynman ―*
## My Expertise:
I have specialized on rapid ml-prototyping and on fast automation of ml-workflows using open-source low-code python libraries. ML-workflows include the development of model prototypes, model deployment and also monitoring data and model drifts over time. Over the last couple of years numerous of such low-code ml-libraries have been published allowing to speed up the ml-workflow exponentially which significantly benefits the productivity of a data scientist or the whole developer team of data engineers, data scientists and ml-engineers. 
## My Offers:
- Explaining data science to both technical and non-technical audiences
- Supporting the specialist department to translate business questions into well defined data science use cases
- Conducting PoC-studies as efficiently and quickly as possible including:
    - Explore, visualize and understand the data
    - Prepare the data for modeling (e.g. data resampling, data imputation,  feature engineering, data transformation, data encoding etc.)
    - Train, tune and test the model
    - Interpret the model (e.g. explain single model decisions), if necessary
    - Analyse and improve model fairness, if necessary 
    - Present the key analytical findings in a clear and understandable way to both technical and non-technical audiences 
- Model deployment (e.g. via fast-api, streamlit or gradio)
- Monitoring data and model drifts over time (e.g. using mlflow)
- Hand over the project to the developer team in form of clean and well documented code